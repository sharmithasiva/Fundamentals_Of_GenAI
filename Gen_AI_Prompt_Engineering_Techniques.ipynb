{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain langchain-core langchain-community langchain-groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjJSz7qmzZ-3",
        "outputId": "e070fd79-1f82-43db-8cd8-34c3dd912bd8"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/495.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m495.8/495.8 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/137.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "groq_api_key = userdata.get(\"GROQ_API_KEY\")\n",
        "\n",
        "llm = ChatGroq(\n",
        "    model=\"llama-3.1-8b-instant\",\n",
        "    temperature=0.2,\n",
        "    groq_api_key=groq_api_key\n",
        ")\n"
      ],
      "metadata": {
        "id": "vzSXXevszaBS"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# =========================\n",
        "# 1. PERSONA PROMPT\n",
        "# =========================\n",
        "persona_prompt = PromptTemplate.from_template(\n",
        "    \"You are a senior computer science professor. Explain overfitting with an example.\"\n",
        ")\n",
        "print(\"Persona Prompt:\\n\", (persona_prompt | llm).invoke({}), \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQIB4P3b4lZI",
        "outputId": "f8632be4-1e51-439e-a67b-09c9a38ea04e"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Persona Prompt:\n",
            " content=\"Overfitting is a fundamental concept in machine learning, and it's essential to understand it to build accurate and reliable models.\\n\\n**What is Overfitting?**\\n\\nOverfitting occurs when a machine learning model is too complex and learns the noise in the training data rather than the underlying patterns. As a result, the model performs well on the training data but poorly on new, unseen data. This is because the model has memorized the training data rather than generalizing from it.\\n\\n**Example:**\\n\\nLet's consider a simple example to illustrate overfitting. Suppose we're building a model to predict the price of a house based on its features, such as the number of bedrooms, square footage, and location.\\n\\nWe collect a dataset of 100 houses with their corresponding features and prices. We split the data into training (80 houses) and testing (20 houses) sets.\\n\\n**Model 1: Simple Linear Regression**\\n\\nWe start by building a simple linear regression model that predicts the price of a house based on the number of bedrooms and square footage. The model is:\\n\\nPrice = β0 + β1 * Bedrooms + β2 * SquareFootage\\n\\nWe train the model on the training data and get a good fit, with a low error rate. The model performs well on the training data, but when we test it on the unseen testing data, it performs poorly, with a high error rate.\\n\\n**Model 2: Overfitting**\\n\\nNext, we build a more complex model that includes additional features, such as the location, number of bathrooms, and type of flooring. The model is:\\n\\nPrice = β0 + β1 * Bedrooms + β2 * SquareFootage + β3 * Location + β4 * Bathrooms + β5 * Flooring\\n\\nWe train the model on the training data and get an even better fit, with an extremely low error rate. However, when we test the model on the unseen testing data, it performs extremely poorly, with a very high error rate.\\n\\n**What Happened?**\\n\\nIn the second model, we added more features to the model, which increased its complexity. As a result, the model learned the noise in the training data, including the specific values of the additional features, rather than the underlying patterns. This is an example of overfitting, where the model is too complex and has memorized the training data rather than generalizing from it.\\n\\n**Solution**\\n\\nTo avoid overfitting, we need to regularize the model, which means adding a penalty term to the loss function to discourage the model from learning the noise in the training data. We can use techniques such as:\\n\\n1. **Regularization**: Add a penalty term to the loss function, such as L1 or L2 regularization.\\n2. **Early Stopping**: Stop training the model when the error on the validation set starts to increase.\\n3. **Data Augmentation**: Increase the size of the training data by generating new examples through transformations.\\n4. **Ensemble Methods**: Combine the predictions of multiple models to reduce overfitting.\\n\\nBy using these techniques, we can build a model that generalizes well to new, unseen data and avoids overfitting.\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 644, 'prompt_tokens': 51, 'total_tokens': 695, 'completion_time': 0.85651655, 'completion_tokens_details': None, 'prompt_time': 0.003224659, 'prompt_tokens_details': None, 'queue_time': 0.049537678, 'total_time': 0.859741209}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_4387d3edbb', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019c21e6-53b1-7b12-9da4-3982d8ce0d01-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 51, 'output_tokens': 644, 'total_tokens': 695} \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 2. ROOT PROMPT\n",
        "# =========================\n",
        "root_prompt = PromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "You are an AI tutor.\n",
        "Always explain step by step using simple language.\n",
        "\n",
        "Explain recursion.\n",
        "\"\"\"\n",
        ")\n",
        "print(\"Root Prompt:\\n\", (root_prompt | llm).invoke({}), \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilX141lz4lVk",
        "outputId": "394c5800-9745-4d97-fda9-2df016b223f7"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root Prompt:\n",
            " content=\"I'd be happy to explain recursion in a simple way.\\n\\n**What is Recursion?**\\n\\nRecursion is a way to solve problems by breaking them down into smaller versions of the same problem. It's like a puzzle where you solve a small part, and then use that solution to solve the next part, and so on.\\n\\n**How Does Recursion Work?**\\n\\nHere's a step-by-step explanation:\\n\\n1. **Start with a problem**: You have a problem that you want to solve, like finding the factorial of a number (e.g., 5!).\\n2. **Break it down**: You break down the problem into smaller versions of the same problem. For example, to find 5!, you can break it down into 5 x 4 x 3 x 2 x 1.\\n3. **Solve the smaller problem**: You solve the smaller problem, like finding 4! (4 x 3 x 2 x 1).\\n4. **Use the solution to solve the next problem**: You use the solution to the smaller problem (4!) to solve the next part of the original problem (5 x 4!).\\n5. **Repeat the process**: You repeat steps 2-4 until you have solved the entire problem.\\n\\n**Example: Factorial**\\n\\nLet's use the example of finding the factorial of 5 (5!).\\n\\n1. Start with the problem: Find 5! (5 x 4 x 3 x 2 x 1).\\n2. Break it down: Find 5 x 4! (4 x 3 x 2 x 1).\\n3. Solve the smaller problem: Find 4! (4 x 3 x 2 x 1) = 24.\\n4. Use the solution to solve the next problem: 5 x 24 = 120.\\n5. Repeat the process: Since we've solved the entire problem, we can stop here.\\n\\n**Recursion in Code**\\n\\nHere's an example of recursion in code:\\n```python\\ndef factorial(n):\\n    if n == 1:  # base case: stop when n is 1\\n        return 1\\n    else:\\n        return n * factorial(n-1)  # recursive call\\n\\nprint(factorial(5))  # output: 120\\n```\\nIn this example, the `factorial` function calls itself (recursively) until it reaches the base case (n == 1).\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 502, 'prompt_tokens': 54, 'total_tokens': 556, 'completion_time': 0.577183613, 'completion_tokens_details': None, 'prompt_time': 0.002562572, 'prompt_tokens_details': None, 'queue_time': 0.048883358, 'total_time': 0.579746185}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ff2b098aaf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019c21e6-5d8d-7183-a889-aa3e2256f62d-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 54, 'output_tokens': 502, 'total_tokens': 556} \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 3. RGC (ROLE–GOAL–CONSTRAINTS)\n",
        "# =========================\n",
        "rgc_prompt = PromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "Role: Machine Learning Instructor\n",
        "Goal: Explain logistic regression\n",
        "Constraints:\n",
        "- Use bullet points\n",
        "- Max 5 points\n",
        "- Beginner friendly\n",
        "\"\"\"\n",
        ")\n",
        "print(\"RGC Prompt:\\n\", (rgc_prompt | llm).invoke({}), \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpQFn-9P48Re",
        "outputId": "bc99918f-4c13-40d2-ab37-0847060e343b"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RGC Prompt:\n",
            " content=\"Welcome to the world of Machine Learning. Today, we're going to explore one of the fundamental algorithms: Logistic Regression.\\n\\n**What is Logistic Regression?**\\n\\nLogistic Regression is a type of supervised learning algorithm used for binary classification problems. It predicts the probability of an event occurring based on a set of input features.\\n\\n**Key Points:**\\n\\n* **Binary Classification**: Logistic Regression is used to predict a binary outcome (0 or 1, yes or no, etc.) based on the input features.\\n* **Probability Estimation**: The algorithm estimates the probability of the positive class (1) occurring, given the input features.\\n* **Sigmoid Function**: The logistic regression model uses the sigmoid function (also known as the logistic function) to map the input features to a probability between 0 and 1.\\n* **Cost Function**: The algorithm minimizes the cost function, which measures the difference between the predicted probabilities and the actual labels.\\n* **Interpretability**: Logistic Regression models are easy to interpret, as the coefficients represent the change in the log-odds of the positive class for a one-unit change in the input feature, while holding all other features constant.\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 236, 'prompt_tokens': 64, 'total_tokens': 300, 'completion_time': 0.325910976, 'completion_tokens_details': None, 'prompt_time': 0.004571964, 'prompt_tokens_details': None, 'queue_time': 0.051688575, 'total_time': 0.33048294}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f757f4b0bf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019c21e6-6ce4-7ce0-887a-55afd437a5ab-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 64, 'output_tokens': 236, 'total_tokens': 300} \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 4. QUESTION REFINEMENT PATTERN\n",
        "# =========================\n",
        "refine_prompt = PromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "Refine the following question to make it clearer and then answer it.\n",
        "\n",
        "Question: Explain training\n",
        "\"\"\"\n",
        ")\n",
        "print(\"Question Refinement:\\n\", (refine_prompt | llm).invoke({}), \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQPdM4PB48vF",
        "outputId": "0b4d9a89-5ea6-491d-b000-f4aa6d853f21"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question Refinement:\n",
            " content='Refined question: What is training, and what are its purposes and benefits?\\n\\nAnswer: Training is the process of acquiring new skills, knowledge, or behaviors through instruction, practice, or experience. It involves the transfer of information, skills, or attitudes from one person to another, with the goal of improving performance, productivity, or efficiency.\\n\\nThe purposes of training include:\\n\\n1. **Knowledge acquisition**: To learn new concepts, theories, or techniques relevant to a particular job or industry.\\n2. **Skill development**: To improve existing skills or acquire new ones, such as communication, problem-solving, or leadership skills.\\n3. **Performance improvement**: To enhance job performance, productivity, or efficiency.\\n4. **Career development**: To advance in a career or prepare for a new role.\\n5. **Compliance**: To meet regulatory or industry requirements.\\n\\nThe benefits of training include:\\n\\n1. **Improved job performance**: Training can lead to increased productivity, efficiency, and accuracy.\\n2. **Increased confidence**: Acquiring new skills and knowledge can boost self-confidence and self-esteem.\\n3. **Better decision-making**: Training can improve critical thinking and problem-solving skills.\\n4. **Enhanced career prospects**: Training can lead to career advancement, promotions, or new job opportunities.\\n5. **Competitive advantage**: Organizations that invest in training can gain a competitive edge in the market.\\n\\nTraining can take various forms, including:\\n\\n1. **Classroom training**: Instructor-led training sessions.\\n2. **Online training**: Web-based training modules or courses.\\n3. **On-the-job training**: Learning by doing, with guidance from experienced colleagues or supervisors.\\n4. **Mentorship**: Pairing with an experienced colleague or mentor for guidance and support.\\n5. **Simulation-based training**: Using simulations or virtual reality to practice and learn new skills.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 369, 'prompt_tokens': 54, 'total_tokens': 423, 'completion_time': 0.524667003, 'completion_tokens_details': None, 'prompt_time': 0.002711858, 'prompt_tokens_details': None, 'queue_time': 0.055938082, 'total_time': 0.527378861}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_4387d3edbb', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019c21e6-74de-7322-a695-aa0b7c919da6-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 54, 'output_tokens': 369, 'total_tokens': 423} \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 5. CHAIN OF THOUGHT PROMPTING\n",
        "# =========================\n",
        "cot_prompt = PromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "Think step by step and explain your reasoning.\n",
        "\n",
        "If an algorithm runs in O(n) time and is called n times,\n",
        "what is the total time complexity?\n",
        "\"\"\"\n",
        ")\n",
        "print(\"Chain of Thought:\\n\", (cot_prompt | llm).invoke({}), \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUJA6DRK48zW",
        "outputId": "50f9e7a9-fd1c-40b5-b405-16adac98f050"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain of Thought:\n",
            " content=\"To determine the total time complexity, let's break it down step by step:\\n\\n1. **Understanding O(n) time complexity**: O(n) time complexity means that the algorithm's running time grows linearly with the size of the input, 'n'. In other words, if the input size doubles, the running time also doubles.\\n\\n2. **Algorithm called n times**: The algorithm is called 'n' times, which means it runs 'n' times in total.\\n\\n3. **Calculating total running time**: Since the algorithm runs in O(n) time each time it's called, and it's called 'n' times, the total running time is the sum of the running times for each call.\\n\\n4. **Sum of O(n) terms**: When you add 'n' terms of O(n), it's equivalent to adding 'n' times the input size, 'n'. This can be represented as n * O(n).\\n\\n5. **Simplifying the expression**: Since 'n' is a constant factor, it can be moved outside the O() notation. Therefore, n * O(n) simplifies to O(n^2).\\n\\nSo, the total time complexity when an algorithm runs in O(n) time and is called 'n' times is O(n^2).\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 262, 'prompt_tokens': 66, 'total_tokens': 328, 'completion_time': 0.368989052, 'completion_tokens_details': None, 'prompt_time': 0.006313366, 'prompt_tokens_details': None, 'queue_time': 0.055349602, 'total_time': 0.375302418}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f757f4b0bf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019c21e6-8384-7001-9458-9945dae5a27d-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 66, 'output_tokens': 262, 'total_tokens': 328} \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 6. COGNITIVE VERIFIER PATTERN\n",
        "# =========================\n",
        "verifier_prompt = PromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "Answer the following question.\n",
        "Then verify your answer and correct any mistakes.\n",
        "\n",
        "Question: Why is binary search O(log n)?\n",
        "\"\"\"\n",
        ")\n",
        "print(\"Cognitive Verifier:\\n\", (verifier_prompt | llm).invoke({}), \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBlxgwG05IUe",
        "outputId": "b60a8e06-e532-4afb-ca52-c20d1eda28bf"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cognitive Verifier:\n",
            " content=\"Binary search is O(log n) because it divides the search space in half with each comparison. \\n\\nHere's a step-by-step explanation:\\n\\n1. Initially, the search space is the entire array of size n.\\n2. The first comparison reduces the search space to n/2.\\n3. The second comparison reduces the search space to n/4.\\n4. The third comparison reduces the search space to n/8, and so on.\\n5. This process continues until the target element is found or the search space is empty.\\n\\nThe key insight is that each comparison reduces the search space by a factor of 2. This is equivalent to a logarithmic reduction, because log2(n) is the number of times you need to divide n by 2 to reach 1.\\n\\nMathematically, we can express this as:\\n\\nlog2(n) = k, where k is the number of comparisons\\n\\n2^k = n, where 2^k is the size of the search space after k comparisons\\n\\nSince each comparison reduces the search space by a factor of 2, the number of comparisons grows logarithmically with the size of the search space. Therefore, binary search has a time complexity of O(log n).\\n\\nVerification and correction:\\n\\nThe above explanation is correct. However, it's worth noting that the actual time complexity of binary search is O(log n) in the worst case, where n is the size of the input array. In the best case, where the target element is already at the first position, binary search takes O(1) time. In the average case, the time complexity is also O(log n).\\n\\nTo verify this, we can analyze the recurrence relation for binary search:\\n\\nT(n) = T(n/2) + O(1)\\n\\nwhere T(n) is the time complexity of binary search on an array of size n.\\n\\nSolving this recurrence relation, we get:\\n\\nT(n) = O(log n)\\n\\nTherefore, the time complexity of binary search is indeed O(log n).\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 408, 'prompt_tokens': 59, 'total_tokens': 467, 'completion_time': 0.556510845, 'completion_tokens_details': None, 'prompt_time': 0.003990084, 'prompt_tokens_details': None, 'queue_time': 0.050956826, 'total_time': 0.560500929}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f757f4b0bf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019c21e6-8f36-7a52-b34d-60605e548e38-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 59, 'output_tokens': 408, 'total_tokens': 467} \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 7. PROVIDE INFO + ASK QUESTION\n",
        "# =========================\n",
        "info_prompt = PromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "Explain overfitting in 3 lines.\n",
        "Then ask one follow-up question to test understanding.\n",
        "\"\"\"\n",
        ")\n",
        "print(\"Provide Info + Ask:\\n\", (info_prompt | llm).invoke({}), \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJszWcu-5K6b",
        "outputId": "1c692272-426c-44d7-dee9-c4b5dc654a36"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Provide Info + Ask:\n",
            " content='Overfitting occurs when a model is too complex and learns the noise in the training data, resulting in poor performance on new, unseen data. This happens when the model is trying to fit the training data too closely, rather than generalizing to the underlying patterns. As a result, the model becomes overly specialized to the training data and fails to make accurate predictions on new data.\\n\\nCan you explain why regularization techniques are often used to prevent overfitting?' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 92, 'prompt_tokens': 55, 'total_tokens': 147, 'completion_time': 0.132658847, 'completion_tokens_details': None, 'prompt_time': 0.002998963, 'prompt_tokens_details': None, 'queue_time': 0.049421254, 'total_time': 0.13565781}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_4387d3edbb', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019c21e6-98e1-7d61-b601-32e36899cae0-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 55, 'output_tokens': 92, 'total_tokens': 147} \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 8. TABULAR FORMAT PROMPTING\n",
        "# =========================\n",
        "table_prompt = PromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "Compare BFS and DFS in a table with columns:\n",
        "Feature | BFS | DFS\n",
        "\"\"\"\n",
        ")\n",
        "print(\"Tabular Format:\\n\", (table_prompt | llm).invoke({}), \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KH-ot-mR5SNO",
        "outputId": "0afe252e-ff7e-4402-f573-3d3859d6daae"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tabular Format:\n",
            " content=\"Here's a comparison of BFS (Breadth-First Search) and DFS (Depth-First Search) in a table:\\n\\n| **Feature** | **BFS** | **DFS** |\\n| --- | --- | --- |\\n| **Traversal Order** | Visits all nodes at a given depth level before moving to the next level | Visits as far as possible along each branch before backtracking |\\n| **Time Complexity** | O(|E| + |V|) where |E| is the number of edges and |V| is the number of vertices | O(|E| + |V|) where |E| is the number of edges and |V| is the number of vertices |\\n| **Space Complexity** | O(|V|) for the queue data structure | O(|V|) for the recursion stack or O(|V|) for the recursion stack |\\n| **Suitability** | Suitable for finding shortest paths, minimum spanning trees, and traversing graphs with a small number of edges | Suitable for finding strongly connected components, topological sorting, and traversing graphs with a large number of edges |\\n| **Implementation** | Uses a queue data structure to keep track of nodes to visit | Uses a stack or recursion to keep track of nodes to visit |\\n| **Example Use Cases** | Finding the shortest path between two nodes in a graph, traversing a maze, or finding the minimum spanning tree of a graph | Finding strongly connected components in a graph, topological sorting, or traversing a graph with a large number of edges |\\n| **Termination** | Terminates when all nodes have been visited or a specific condition is met | Terminates when a specific condition is met or when the recursion stack is empty |\\n\\nNote: |E| and |V| represent the number of edges and vertices in the graph, respectively.\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 378, 'prompt_tokens': 51, 'total_tokens': 429, 'completion_time': 0.465316348, 'completion_tokens_details': None, 'prompt_time': 0.003117828, 'prompt_tokens_details': None, 'queue_time': 0.050948822, 'total_time': 0.468434176}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f757f4b0bf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019c21e6-a19b-76d0-913f-f04944ebfd8c-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 51, 'output_tokens': 378, 'total_tokens': 429} \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 9. FILL-IN-THE-BLANK PROMPTING\n",
        "# =========================\n",
        "fill_prompt = PromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "Fill in the blanks:\n",
        "\n",
        "Precision = ___ / ___\n",
        "Recall = ___ / ___\n",
        "\"\"\"\n",
        ")\n",
        "print(\"Fill in the Blanks:\\n\", (fill_prompt | llm).invoke({}), \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TW_QuhY5SJz",
        "outputId": "df2f62c1-1016-40b9-9666-71f43cb9f07c"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fill in the Blanks:\n",
            " content='Precision = True Positives / (True Positives + False Positives)\\nRecall = True Positives / (True Positives + False Negatives)\\n\\nIn other words:\\n\\n- Precision is the ratio of correctly identified positive instances (True Positives) to the total number of instances that were identified as positive (True Positives + False Positives).\\n- Recall is the ratio of correctly identified positive instances (True Positives) to the total number of actual positive instances (True Positives + False Negatives).' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 103, 'prompt_tokens': 53, 'total_tokens': 156, 'completion_time': 0.1366794, 'completion_tokens_details': None, 'prompt_time': 0.002759327, 'prompt_tokens_details': None, 'queue_time': 0.048969853, 'total_time': 0.139438727}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_4387d3edbb', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019c21e6-ab5d-7200-9532-da415621552a-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 53, 'output_tokens': 103, 'total_tokens': 156} \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 10. ZERO-SHOT PROMPTING\n",
        "# =========================\n",
        "zero_shot_prompt = PromptTemplate.from_template(\n",
        "    \"Classify the sentiment: The lecture was very interesting.\"\n",
        ")\n",
        "print(\"Zero-Shot:\\n\", (zero_shot_prompt | llm).invoke({}), \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRQ6zdwJ5SHg",
        "outputId": "197804fd-df25-4b4d-ac39-9927caf05d04"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zero-Shot:\n",
            " content='The sentiment of the statement \"The lecture was very interesting\" is positive. The word \"interesting\" has a positive connotation, indicating that the speaker enjoyed or found the lecture engaging and worthwhile.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 40, 'prompt_tokens': 46, 'total_tokens': 86, 'completion_time': 0.076420821, 'completion_tokens_details': None, 'prompt_time': 0.002789167, 'prompt_tokens_details': None, 'queue_time': 0.049396629, 'total_time': 0.079209988}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_4387d3edbb', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019c21e6-b54a-7843-bf45-a1ec370a77d7-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 46, 'output_tokens': 40, 'total_tokens': 86} \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 11. ONE-SHOT PROMPTING\n",
        "# =========================\n",
        "one_shot_prompt = PromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "Example:\n",
        "Input: Bad experience → Negative\n",
        "\n",
        "Classify:\n",
        "Input: Amazing teaching\n",
        "\"\"\"\n",
        ")\n",
        "print(\"One-Shot:\\n\", (one_shot_prompt | llm).invoke({}), \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEeBZ2zj5SE8",
        "outputId": "d6aeb230-6a79-43b1-c59d-ea6fab2d77b7"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-Shot:\n",
            " content='Based on the given example, I would classify the input as:\\n\\nInput: Amazing teaching → Positive\\n\\nThis is because the word \"amazing\" has a positive connotation, and the word \"teaching\" is a neutral or positive topic.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 50, 'prompt_tokens': 52, 'total_tokens': 102, 'completion_time': 0.070790443, 'completion_tokens_details': None, 'prompt_time': 0.002510134, 'prompt_tokens_details': None, 'queue_time': 0.050360105, 'total_time': 0.073300577}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ff2b098aaf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019c21e6-bd18-7321-a0d1-3c2077fde749-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 52, 'output_tokens': 50, 'total_tokens': 102} \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 12. FEW-SHOT PROMPTING\n",
        "# =========================\n",
        "few_shot_prompt = PromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "Input: Worst product ever → Negative\n",
        "Input: It was okay → Neutral\n",
        "Input: Loved it → Positive\n",
        "\n",
        "Classify:\n",
        "Input: The app works perfectly\n",
        "\"\"\"\n",
        ")\n",
        "print(\"Few-Shot:\\n\", (few_shot_prompt | llm).invoke({}), \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6g8AHur5ZV4",
        "outputId": "91a91fe7-8d4e-4248-ae19-4f4bd423de74"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Few-Shot:\n",
            " content='Based on the input \"The app works perfectly\", I would classify it as Positive.\\n\\nHere\\'s the reasoning:\\n\\n- The word \"perfectly\" has a very positive connotation, implying that the app is functioning flawlessly.\\n- The word \"works\" is a neutral term, but in this context, it\\'s used to emphasize the app\\'s functionality.\\n- The overall tone of the sentence is one of satisfaction and approval.\\n\\nSo, the classification is Positive, similar to the input \"Loved it\".' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 102, 'prompt_tokens': 68, 'total_tokens': 170, 'completion_time': 0.135525234, 'completion_tokens_details': None, 'prompt_time': 0.003765428, 'prompt_tokens_details': None, 'queue_time': 0.050343989, 'total_time': 0.139290662}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f757f4b0bf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019c21e6-d0f2-7380-b86a-7c9d6a387ca0-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 68, 'output_tokens': 102, 'total_tokens': 170} \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 13. USER INPUT (GENERIC)\n",
        "# =========================\n",
        "user_question = input(\"Enter your own question: \")\n",
        "\n",
        "user_prompt = PromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "print(\"User Input Response:\\n\", (user_prompt | llm).invoke({\"question\": user_question}))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chFrr7Xa5dZt",
        "outputId": "7b621e4c-57c8-4f42-b5cb-3d63e00e4883"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your own question: you are ugly. classify the sentiiment\n",
            "User Input Response:\n",
            " content='The sentiment of the statement \"you are ugly\" is negative. It is a direct insult that aims to hurt or offend the person being addressed.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 49, 'total_tokens': 79, 'completion_time': 0.069343541, 'completion_tokens_details': None, 'prompt_time': 0.002366271, 'prompt_tokens_details': None, 'queue_time': 0.050159696, 'total_time': 0.071709812}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f757f4b0bf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019c21e7-3377-7c42-a6ee-63f30c680c7b-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 49, 'output_tokens': 30, 'total_tokens': 79}\n"
          ]
        }
      ]
    }
  ]
}